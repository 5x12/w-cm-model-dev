#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Fri Nov 19 12:44:00 2021@author: skhalilbekov"""import copyimport datetimeimport jsonimport pickleimport numpy as npimport pandas as pdimport boximport yamlfrom typing import Text, Dictdef sql_from_file(filename: str) -> str:    with open(filename, 'r') as f:        return f.read()def save_json(file, file_path, encoding='utf-8', ensure_ascii=False):    """function to save file as json"""    with open(file_path, "w", encoding=encoding) as jsonfile:            json.dump(file, jsonfile, ensure_ascii=ensure_ascii)        def read_json(file_path: str, file_dir: str, encoding='utf-8'):    """function to read file as json"""    with open(file_dir + file_path, encoding = encoding) as f:         json_file = json.load(f)    return json_filedef save_pickle(file, file_path):    """function to save files to pickle (mostly to store ML models)"""    with open(file_path, 'wb') as f:        pickle.dump(file, f)        def load_pickle(file_path):    """function to save files to pickle (mostly to store ML models)"""    with open(file_path, 'rb') as f:        pickle_file = pickle.load(f)    return pickle_filedef read_file(file_path, sheetname = 0, dtypes = None,              skiprows = None, format = 'fixed', nrows = None):    """    function to read files in most common formats    :path: relative path to file    :root_path: root path of the local machine    :dtypes: specify types for columns in case of excel files    :return: df    """    # excel formats    if any(file_type in file_path for file_type in ['xlsx', 'xls', 'xlsb']):        output = pd.read_excel(file_path, sheet_name=sheetname,                               dtype=dtypes, skiprows = skiprows)    elif any(file_type in file_path for file_type in ['csv']):        output = pd.read_csv(file_path, skiprows = skiprows,                             nrows = nrows, dtype = dtypes)    # hdf files    elif any(file_type in file_path for file_type in ['hd5', 'h5']):        output = pd.read_hdf(file_path, key = 'df')            # parquet    elif any(file_type in file_path for file_type in ['parquet']):        output = pd.read_parquet(file_path, engine = 'pyarrow')            else:        raise ValueError('Wrong data type is specified {}. Please, double check your file extension'.format(file_path))            return outputdef save_file(file, file_path, format = 'fixed'):    """    function to save files in most common formats    :path: relative path to file    :root_path: root path of the local machine    :dtypes: specify types for columns in case of excel files    :return: df    """    # excel formats    if any(file_type in file_path for file_type in ['xlsx', 'xls', 'xlsb']):        file.to_excel(file_path)            elif any(file_type in file_path for file_type in ['csv']):        file.to_csv(file_path)     # hdf files    elif any(file_type in file_path for file_type in ['hd5', 'h5']):        file.to_hdf(file_path, key = 'df', format = format)            # parquet    elif any(file_type in file_path for file_type in ['parquet']):        file.to_parquet(file_path, engine = 'pyarrow')            else:        raise ValueError('Wrong data type is specified {}. Please, double check your file extension'.format(file_path))def mean_absolute_percentage_error(y_true, y_pred):        y_true, y_pred = np.array(y_true), np.array(y_pred)    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100def load_config(path: Text) -> box.ConfigBox:    with open(path) as f:        config = yaml.safe_load(f)        config = box.ConfigBox(config)    return configdef load_long_data(conn, query: Text, query_params: Dict, max_sql_period: int, datetime_fmt: str):    """    Функция, позволяющая загружать данные частями. Если временной промежуток больше max_sql_period дней,    то временной промежуток [dt_from_str, dt_to_str) разбивается на части, каждый длиной в max_sql_period.    Если же временной промежуток [dt_from_str, dt_to_str) меньше max_sql_period дней, то данные загружается за один раз.    """    dt_from = datetime.datetime.strptime(query_params.get("dt_from"), datetime_fmt)    dt_to = datetime.datetime.strptime(query_params.get("dt_to"), datetime_fmt)    delta_in_days = (dt_to - dt_from).days    if delta_in_days <= max_sql_period:        query = query.format(**query_params)        return conn.fetch(query)    else:        n_queries = delta_in_days / max_sql_period        data = pd.DataFrame()        query_params_for_chunk = copy.deepcopy(query_params)        for i in np.arange(0, int(np.floor(n_queries))):            dt_from_str_ = (dt_from + datetime.timedelta(days=int(i * max_sql_period))).strftime(datetime_fmt)            dt_to_str_ = (dt_from + datetime.timedelta(days=int((i + 1) * max_sql_period))).strftime(datetime_fmt)            query_params_for_chunk["dt_from"] = dt_from_str_            query_params_for_chunk["dt_to"] = dt_to_str_            data_chunk = conn.fetch(query.format(**query_params_for_chunk))            data = pd.concat((data, data_chunk))        query_params_for_chunk["dt_from"] = dt_to_str_        query_params_for_chunk["dt_to"] = query_params["dt_to"]        if n_queries > np.floor(n_queries):            # Load data for last period less than MAX_SQL_PERIOD            data_chunk = conn.fetch(query.format(**query_params_for_chunk))            data = pd.concat((data, data_chunk))        return datadef generate_ful_dt_df(predictions: pd.DataFrame, start_date: str,  end_date: str):    """    adhoc function to extrapolate values forward/back if some date is missing    """    df = pd.DataFrame(pd.date_range(start = start_date, end = end_date),                      columns = ['ds'])    df = pd.merge(df, predictions, how = 'left', on = 'ds')    df = df.fillna(method = 'ffill')        return df            